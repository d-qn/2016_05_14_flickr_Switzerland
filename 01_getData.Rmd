---
title: "flickr map Switzerland"
author: "Duc-Quang Nguyen"
date: "14 May 2016"
output: html_document
---

## FlickR
* [timelyportfolio R and flickr](http://timelyportfolio.github.io/rCharts_Rflickr/iso_httr.html)
* [flickr color analysis](http://beautifuldata.net/2013/05/color-analysis-of-flickr-images/)
* [](https://github.com/furukama/flickr/blob/master/flickr_get.R)
* [my API page](https://www.flickr.com/services/apps/by/112725067@N03)
* [official doc API](https://www.flickr.com/services/api/)

* [analyze-instagram-r](http://thinktostart.com/analyze-instagram-r/)
# https://www.instagram.com/developer/endpoints/locations/

```{r setup, include=FALSE}
require(httr)
library(RCurl)
library(magrittr)
library(dplyr)
library(jsonlite)
library(parallel)

startDate <- as.Date("2014-01-01")
endDate <- as.Date("2016-05-18") #Sys.Date()
freqTime <- "week"
  
getPics <- F
getInfoPics <- F

# Use procedure in http://timelyportfolio.github.io/rCharts_Rflickr/iso_httr.html works!
# save(api_key, secret, flickr.app, flickr.endpoint, tok, file = "~/swissinfo/_helpers/secrets.Rdata")

load("~/swissinfo/_helpers/secrets.Rdata")


if(getPics) {
  # flickr search API function
  flickrSearch <- function(
    bbox, 
    content_type = 1, 
    min_taken_date = format( Sys.Date() - 7, "%Y-%m-%d"), 
    max_taken_date = format( Sys.Date(), "%Y-%m-%d"), 
    api_key = api_key, 
    tok = tok
  ) {

  search <-   GET(url=sprintf(
      "https://api.flickr.com/services/rest/?method=flickr.photos.search&api_key=%s&bbox=%s&content_type=%s&min_taken_date=%s&max_taken_date=%s&format=json&nojsoncallback=1"
      , api_key
      , bbox
      , content_type
      , min_taken_date
      , max_taken_date
      , tok$credentials$oauth_token
      )
    ) %>%
      content( as = "text", encoding="UTF-8") %>%
      jsonlite::fromJSON () 
  
     stopifnot(search[[2]] == "ok")
    # subset the list to get only the relevant data
    search %$% photos %$% photo
}

  time.frames <- data.frame(
    min_taken_date = seq(startDate, endDate, freqTime)
  )
  time.frames$max_taken_date = c(time.frames[-1, ] -1, Sys.Date() + 1)
  
  # To define bbox, use http://bboxfinder.com/#45.859412,5.844727,47.524620,10.755615
  bbox <- paste(c(5.899658,45.836454,10.541382,47.735629), collapse = ",")

  pics <- do.call(rbind, lapply(1:nrow(time.frames), function(i) {
    cat("\n", i)
    flickrSearch(
      bbox = bbox, 
      min_taken_date = time.frames[i,'min_taken_date'],
      max_taken_date = time.frames[i,'max_taken_date'], 
      api_key = api_key, 
      tok = tok)
  }))
  pics <- pics %>% select(-isfriend, -isfamily)
  write.csv(pics, file = paste0("data/", startDate, "_", endDate, "_by", freqTime, ".csv"), row.names = F)
} else {
  pics <- read.csv(file=paste0("data/", startDate, "_", endDate, "_by", freqTime, ".csv"), stringsAsFactors = F)
}

stopifnot(!any(duplicated(pics)))


if(getInfoPics) {
  # flickr API get info 
getInfo <- function(id, api_key = api_key, tok = tok) {
    call <- GET(url=sprintf(
      "https://api.flickr.com/services/rest/?method=flickr.photos.getInfo&api_key=%s&photo_id=%s&format=json&nojsoncallback=1"
      , api_key
      , id
      , tok$credentials$oauth_token
    ))  %>%
      content( as = "text", encoding="UTF-8") %>%
      jsonlite::fromJSON () 
    
    if(call[[2]] == "ok") {
    result <- call %$% photo
    data.frame(
      id = as.numeric(result$id), 
      lat = as.numeric(result$location$latitude), 
      lon = as.numeric(result$location$longitude), 
      locality = if(is.null(result$location$locality$`_content`)) "" else result$location$locality$`_content`, 
      county = if(is.null(result$location$county$`_content`)) "" else result$location$county$`_content`, 
      region = if(is.null(result$location$region$`_content`)) "" else result$location$region$`_content`, 
      country = if(is.null(result$location$country$`_content`)) "" else result$location$country$`_content`,
      isFavorite = as.numeric(result$isfavorite),
      dateTaken = as.character(result$dates$taken),
      views = as.numeric(result$views),
      url = as.character(result$url$url$`_content`)
    )     
    } else {
      warning("\nAPI call for ", id, " failed!")
      NULL
    }
}  
    
  # http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/
  # Calculate the number of cores
  # Initiate cluster
  cl <- makeCluster(detectCores(), outfile ="")
  clusterExport(cl=cl, varlist=c("pics", "getInfo", "api_key", "tok", "%>%", "GET", "content", "%$%"))
  
  infos <- do.call(rbind, parLapply(cl, 1:nrow(pics), function(i) {
     cat("\n", i, "/", nrow(pics), "\t", pics[i, 'id'])
     getInfo(pics[i, 'id'], api_key = api_key, tok= tok) 
  }))
  stopCluster(cl)
  
  pici <- right_join(pics, infos)
  
  write.csv(pici, file = paste0("data/", startDate, "_", endDate, "_by", freqTime, "_info.csv"), row.names = F)
} else {
  pici <- read.csv(file = paste0("data/", startDate, "_", endDate, "_by", freqTime, "_info.csv"), stringsAsFactors = F)
}









#discard pics not taken in Switzerland
pig <- picg %>% filter(country == "Switzerland")
library(leaflet)
library(swiMap)
require(rgdal)
require(rgeos)

path.ch <- getPathShp('CH')
co <- readOGR(path.ch, layer = 'country')
lakes <- readOGR(path.ch, layer = 'lakes')
co <- spTransform(co, CRS("+init=epsg:4326"))
lakes <- spTransform(lakes, CRS("+init=epsg:4326"))

basem <- leaflet(co) %>%  addPolygons(
    stroke = TRUE, fillOpacity = 0, smoothFactor = 0.5, weight = 1
  )

leaflet() %>% 
  addCircleMarkers(data = pig,
    lng = ~lon, lat = ~lat, radius = 1, 
    stroke = FALSE, fillOpacity = 0.8, color = "#336666") %>%
  addPolygons(data = co, stroke = TRUE, fillOpacity = 0, 
    smoothFactor = 0.5, weight = 1)  %>%
  addPolygons(data = lakes, stroke = FALSE, fillOpacity = 1, 
    smoothFactor = 0.5, weight = 1)   

  


flickr.photos.getInfo





flickr.photos.geo.getLocation




```