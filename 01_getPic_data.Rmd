---
title: "flickr map Switzerland"
author: "Duc-Quang Nguyen"
date: "14 May 2016"
output: html_document
---

## FlickR
* [timelyportfolio R and flickr](http://timelyportfolio.github.io/rCharts_Rflickr/iso_httr.html)
* [flickr color analysis](http://beautifuldata.net/2013/05/color-analysis-of-flickr-images/)
* [](https://github.com/furukama/flickr/blob/master/flickr_get.R)
* [my API page](https://www.flickr.com/services/apps/by/112725067@N03)
* [official doc API](https://www.flickr.com/services/api/)

* [analyze-instagram-r](http://thinktostart.com/analyze-instagram-r/)
# https://www.instagram.com/developer/endpoints/locations/

```{r setup, include=FALSE}
require(httr)
library(RCurl)
library(magrittr)
library(dplyr)
library(jsonlite)
library(parallel)

startDate <- as.Date("2014-01-01")
endDate <- as.Date("2016-05-18") #Sys.Date()
freqTime <- "week"
  
getPics <- F
getInfoPics <- F
getFavs <- F ## TODO

# Use procedure in http://timelyportfolio.github.io/rCharts_Rflickr/iso_httr.html works!
# save(api_key, secret, flickr.app, flickr.endpoint, tok, file = "~/swissinfo/_helpers/secrets.Rdata")

load("~/swissinfo/_helpers/secrets.Rdata")


if(getPics) {
  # flickr search API function
  flickrSearch <- function(
    bbox, 
    content_type = 1, 
    min_taken_date = format( Sys.Date() - 7, "%Y-%m-%d"), 
    max_taken_date = format( Sys.Date(), "%Y-%m-%d"), 
    api_key = api_key, 
    tok = tok
  ) {

  search <-   GET(url=sprintf(
      "https://api.flickr.com/services/rest/?method=flickr.photos.search&api_key=%s&bbox=%s&content_type=%s&min_taken_date=%s&max_taken_date=%s&format=json&nojsoncallback=1"
      , api_key
      , bbox
      , content_type
      , min_taken_date
      , max_taken_date
      , tok$credentials$oauth_token
      )
    ) %>%
      content( as = "text", encoding="UTF-8") %>%
      jsonlite::fromJSON () 
  
     stopifnot(search[[2]] == "ok")
    # subset the list to get only the relevant data
    search %$% photos %$% photo
}

  time.frames <- data.frame(
    min_taken_date = seq(startDate, endDate, freqTime)
  )
  time.frames$max_taken_date = c(time.frames[-1, ] -1, Sys.Date() + 1)
  
  # To define bbox, use http://bboxfinder.com/#45.809658,5.745850,47.813155,10.563354
  bbox <- paste(c(5.745850,45.809658,10.563354,47.813155), collapse = ",")

  pics <- do.call(rbind, lapply(1:nrow(time.frames), function(i) {
    cat("\n", i)
    flickrSearch(
      bbox = bbox, 
      min_taken_date = time.frames[i,'min_taken_date'],
      max_taken_date = time.frames[i,'max_taken_date'], 
      api_key = api_key, 
      tok = tok)
  }))
  pics <- pics %>% select(-isfriend, -isfamily)
  write.csv(pics, file = paste0("data/", startDate, "_", endDate, "_by", freqTime, ".csv"), row.names = F)
} else {
  pics <- read.csv(file=paste0("data/", startDate, "_", endDate, "_by", freqTime, ".csv"), stringsAsFactors = F)
}

stopifnot(!any(duplicated(pics)))


if(getInfoPics) {
  # flickr API get info 
getInfo <- function(id, api_key = api_key, tok = tok) {
    call <- GET(url=sprintf(
      "https://api.flickr.com/services/rest/?method=flickr.photos.getInfo&api_key=%s&photo_id=%s&format=json&nojsoncallback=1"
      , api_key
      , id
      , tok$credentials$oauth_token
    ))  %>%
      content( as = "text", encoding="UTF-8") %>%
      jsonlite::fromJSON () 
    
    if(call[[2]] == "ok") {
    result <- call %$% photo
    data.frame(
      id = as.numeric(result$id), 
      lat = as.numeric(result$location$latitude), 
      lon = as.numeric(result$location$longitude), 
      locality = if(is.null(result$location$locality$`_content`)) "" else result$location$locality$`_content`, 
      county = if(is.null(result$location$county$`_content`)) "" else result$location$county$`_content`, 
      region = if(is.null(result$location$region$`_content`)) "" else result$location$region$`_content`, 
      country = if(is.null(result$location$country$`_content`)) "" else result$location$country$`_content`,
      isFavorite = as.numeric(result$isfavorite),
      dateTaken = as.character(result$dates$taken),
      views = as.numeric(result$views),
      url = as.character(result$url$url$`_content`)
    )     
    } else {
      warning("\nAPI call for ", id, " failed!")
      NULL
    }
}  
    
  # http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/
  # Calculate the number of cores
  # Initiate cluster
  cl <- makeCluster(detectCores(), outfile ="")
  clusterExport(cl=cl, varlist=c("pics", "getInfo", "api_key", "tok", "%>%", "GET", "content", "%$%"))
  
  infos <- do.call(rbind, parLapply(cl, 1:nrow(pics), function(i) {
     cat("\n", i, "/", nrow(pics), "\t", pics[i, 'id'])
     getInfo(pics[i, 'id'], api_key = api_key, tok= tok) 
  }))
  stopCluster(cl)
  
  pici <- right_join(pics, infos)
  
  write.csv(pici, file = paste0("data/", startDate, "_", endDate, "_by", freqTime, "_info.csv"), row.names = F)
} else {
  pici <- read.csv(file = paste0("data/", startDate, "_", endDate, "_by", freqTime, "_info.csv"), stringsAsFactors = F)
}

if(getFavs) {
  # get favorites count, a different API call!!!
  # https://www.flickr.com/services/api/flickr.photos.getFavorites.html
  flickr.photos.getFavorites  
  
}



#discard pics not taken in Switzerland
pig <- pici %>% filter(country == "Switzerland")
library(leaflet)
library(htmltools)
library(swiMap)
library(swiRcharts)
require(rgdal)
require(rgeos)
require(maptools)

path.ch <- getPathShp('CH', year = 2014)
co <- readOGR(path.ch, layer = 'country')
lakes <- readOGR(path.ch, layer = 'lakes')
mu <- readOGR(path.ch, layer = 'municipalities')

co <- spTransform(co, CRS("+init=epsg:4326"))
lakes <- spTransform(lakes, CRS("+init=epsg:4326"))
mu <- spTransform(mu, CRS("+init=epsg:4326"))
mu.df <- formatShp(mu)


# Find in which polygon each geocoordinates fall in !!!
# https://andybeger.com/2014/03/29/associating-points-with-polygons-in-r/
coordinates(pig) <- ~ lon + lat
proj4string(pig) <- proj4string(mu)
pig <- cbind(pig, over(pig, mu) %>% select(BFS_NUMMER))

sum(is.na(pig$BFS_NUMMER))


basem_url <- 'http://{s}.basemaps.cartocdn.com/dark_all/{z}/{x}/{y}.png'
basem_attribution <- '&copy; <a href="http://www.openstreetmap.org/copyright">OpenStreetMap</a> &copy; <a href="http://cartodb.com/attributions">CartoDB</a>'

popup <- paste0("<strong>", pig$locality, "</strong><br>", htmlLink(pig$url, "picture"))
  
map <- leaflet(height = 900) %>% addTiles(urlTemplate = basem_url, attribution = basem_attribution)  %>%
  addCircleMarkers(data = pig,
    lng = ~lon, lat = ~lat, radius = 0.8, 
    stroke = FALSE, fillOpacity = 0.3, color = '#996666', popup = popup)


save_html(
  tags$html(
    tags$head(includeHTML("styles.html")),
    tags$body(    
      div(class="graphic", map),
      HTML('<script type="text/javascript" src="https://www.swissinfo.ch/static/Themes/basic/js/vendor/iframeResizer.contentWindow.3.5.3.min.js"></script>')
    )
  ), 
file = "flickr_Swiss_test.html", libdir = "js")     

original <- list.files("js", "leaflet.css", full.names = T, recursive = T)
file.copy(list.files(system.file("extdata", package="swiRcharts"), 'leaflet.css', full.names = T), original, overwrite = T)
    
    

    

flickr.photos.geo.getLocation




```