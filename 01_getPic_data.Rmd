---
title: "flickr map Switzerland"
author: "Duc-Quang Nguyen"
date: "14 May 2016"
output: html_document
---

## FlickR
* [timelyportfolio R and flickr](http://timelyportfolio.github.io/rCharts_Rflickr/iso_httr.html)
* [flickr color analysis](http://beautifuldata.net/2013/05/color-analysis-of-flickr-images/)
* [](https://github.com/furukama/flickr/blob/master/flickr_get.R)
* [my API page](https://www.flickr.com/services/apps/by/112725067@N03)
* [official doc API](https://www.flickr.com/services/api/)

```{r setup, include=FALSE}
require(httr)
library(RCurl)
library(magrittr)
library(dplyr)
library(jsonlite)
library(parallel)

startDate <- as.Date("2014-01-01")
endDate <- as.Date("2016-07-14") #Sys.Date()
freqTime <- "week"
  
getPics <- F
getInfoPics <- F
getFavs <- F 

# Use procedure in http://timelyportfolio.github.io/rCharts_Rflickr/iso_httr.html works!
# save(api_key, secret, flickr.app, flickr.endpoint, tok, file = "~/swissinfo/_helpers/secrets.Rdata")

load("~/swissinfo/_helpers/secrets.Rdata")

if(getPics) {
  # flickr search API function
  flickrSearch <- function(
    bbox, 
    content_type = 1, 
    min_taken_date = format( Sys.Date() - 7, "%Y-%m-%d"), 
    max_taken_date = format( Sys.Date(), "%Y-%m-%d"), 
    api_key = api_key, 
    tok = tok
  ) {

  search <-   GET(url=sprintf(
      "https://api.flickr.com/services/rest/?method=flickr.photos.search&api_key=%s&bbox=%s&content_type=%s&min_taken_date=%s&max_taken_date=%s&format=json&nojsoncallback=1"
      , api_key
      , bbox
      , content_type
      , min_taken_date
      , max_taken_date
      , tok$credentials$oauth_token
      )
    ) %>%
      content( as = "text", encoding="UTF-8") %>%
      jsonlite::fromJSON () 
  
     stopifnot(search[[2]] == "ok")
    # subset the list to get only the relevant data
    search %$% photos %$% photo
}

  time.frames <- data.frame(
    min_taken_date = seq(startDate, endDate, freqTime)
  )
  time.frames$max_taken_date = c(time.frames[-1, ] -1, Sys.Date() + 1)
  
  # To define bbox, use http://bboxfinder.com/#45.809658,5.745850,47.813155,10.563354
  bbox <- paste(c(5.745850,45.809658,10.563354,47.813155), collapse = ",")

  pics <- do.call(rbind, lapply(1:nrow(time.frames), function(i) {
    cat("\n", i)
    flickrSearch(
      bbox = bbox, 
      min_taken_date = time.frames[i,'min_taken_date'],
      max_taken_date = time.frames[i,'max_taken_date'], 
      api_key = api_key, 
      tok = tok)
  }))
  pics <- pics %>% select(-isfriend, -isfamily)
  write.csv(pics, file = paste0("data/", startDate, "_", endDate, "_by", freqTime, ".csv"), row.names = F)
} else {
  pics <- read.csv(file=paste0("data/", startDate, "_", endDate, "_by", freqTime, ".csv"), stringsAsFactors = F)
}

stopifnot(!any(duplicated(pics)))


if(getInfoPics) {
  # flickr API get info 
  getInfo <- function(id, api_key = api_key, tok = tok) {
    call <- GET(url=sprintf(
      "https://api.flickr.com/services/rest/?method=flickr.photos.getInfo&api_key=%s&photo_id=%s&format=json&nojsoncallback=1"
      , api_key
      , id
      , tok$credentials$oauth_token
    ))  %>%
      content( as = "text", encoding="UTF-8") %>%
      jsonlite::fromJSON () 
    
    if(call[[2]] == "ok") {
      result <- call %$% photo
      data.frame(
        id = as.numeric(result$id), 
        lat = as.numeric(result$location$latitude), 
        lon = as.numeric(result$location$longitude), 
        locality = if(is.null(result$location$locality$`_content`)) "" else result$location$locality$`_content`, 
        county = if(is.null(result$location$county$`_content`)) "" else result$location$county$`_content`, 
        region = if(is.null(result$location$region$`_content`)) "" else result$location$region$`_content`, 
        country = if(is.null(result$location$country$`_content`)) "" else result$location$country$`_content`,
        isFavorite = as.numeric(result$isfavorite),
        dateTaken = as.character(result$dates$taken),
        views = as.numeric(result$views),
        url = as.character(result$url$url$`_content`)
      )     
    } else {
      warning("\nAPI call for ", id, " failed!")
      NULL
    }
  }  
  
  # http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/
  # Calculate the number of cores
  # Initiate cluster
  cl <- makeCluster(detectCores(), outfile ="")
  clusterExport(cl=cl, varlist=c("pics", "getInfo", "api_key", "tok", "%>%", "GET", "content", "%$%"))
  
  infos <- do.call(rbind, parLapply(cl, 1:nrow(pics), function(i) {
    cat("\n", i, "/", nrow(pics), "\t", pics[i, 'id'])
    getInfo(pics[i, 'id'], api_key = api_key, tok= tok) 
  }))
  stopCluster(cl)
  
  pics$id <- as.numeric(pics$id)
  pici <- right_join(pics, infos) %>% select(-isFavorite, -ispublic)
  
  write.csv(pici, file = paste0("data/", startDate, "_", endDate, "_by", freqTime, "_info.csv"), row.names = F)
} else {
  pici <- read.csv(file = paste0("data/", startDate, "_", endDate, "_by", freqTime, "_info.csv"), stringsAsFactors = F)
}

if(getFavs) {
  # get favorites count, a different API call!!!
  # https://www.flickr.com/services/api/flickr.photos.getFavorites.html  --> flickr.photos.getFavorites  
   # flickr API get info 
  getFav <- function(id, api_key = api_key, tok = tok) {
    call <- GET(url=sprintf(
      "https://api.flickr.com/services/rest/?method=flickr.photos.getFavorites&api_key=%s&photo_id=%s&format=json&nojsoncallback=1"
      , api_key
      , id
      , tok$credentials$oauth_token
    ))  %>%
      content( as = "text", encoding="UTF-8") %>%
      jsonlite::fromJSON () 
    
    if(call[[2]] == "ok") {
      result <- call %$% photo
      data.frame(
        id = as.numeric(result$id), 
        faveCount = as.numeric(result$total)
      )     
    } else {
      warning("\nAPI call for ", id, " failed!")
      NULL
    }
  }  
  
  # http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/
  # Calculate the number of cores
  # Initiate cluster
  cl <- makeCluster(detectCores(), outfile ="")
  clusterExport(cl=cl, varlist=c("pici", "getFav", "api_key", "tok", "%>%", "GET", "content", "%$%"))
  
  favs <- do.call(rbind, parLapply(cl, 1:nrow(pici), function(i) {
    cat("\n", i, "/", nrow(pici), "\t", pici[i, 'id'])
    getFav(pici[i, 'id'], api_key = api_key, tok = tok) 
  }))
  stopCluster(cl)
  
  picif <- right_join(pici, favs)
  
  write.csv(picif, file = paste0("input/", startDate, "_", endDate, "_by", freqTime, "_faved.csv"), row.names = F)
} else {
  picif <- read.csv(file = paste0("input/", startDate, "_", endDate, "_by", freqTime, "_faved.csv"), stringsAsFactors = F)
}

#discard pics not taken in Switzerland

## get only grid points within Switzerland 
#http://www.inside-r.org/packages/cran/sp/docs/point.in.polygon
require(sp)
require("maps")
world <- map_data("world")
ch <- world[which(world$region == "Switzerland"),]
ch.map <- ggplot(ch, aes(x = long, y = lat, group = group)) + 
  geom_path() + coord_equal() + theme_minimal()

picifs <- picif[which(point.in.polygon(picif$lon, picif$lat, ch$long, ch$lat) == 1),]

write.csv(picifs, file = paste0("input/", startDate, "_", endDate, "_by", freqTime, "_faved_CH.csv"), row.names = F)

# visual check
ch.map + geom_point(data = picifs, aes(x = lon, y = lat, group = 1), alpha = 0.3, size = 0.1) 

```